{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5564343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BinaryClassificationModel, Classifier\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,Subset,random_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93fba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18432652",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "052fc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data.csv')\n",
    "\n",
    "data_x = data.iloc[:,:-1]\n",
    "data_y = data.iloc[:,-1]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# 데이터 표준화\n",
    "standardized_data_x = scaler.fit_transform(data_x)\n",
    "data_s_x = pd.DataFrame(standardized_data_x, columns = data_x.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_s_x, data_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a109b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor로 변환\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data_x, data_y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train,dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_list(),dtype = torch.float32).squeeze(-1)  # y를 (1000, 1) 크기로 변환\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test,dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_list(),dtype = torch.float32).squeeze(-1)  # y를 (1000, 1) 크기로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d681c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SingleHeadSelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.keys = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.queries = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(query)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"qd,kd->qk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=1)\n",
    "        out = torch.einsum(\"qk,vd->qd\", [attention, values])\n",
    "        out = self.fc_out(out)\n",
    "        return out, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab3b268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachinePredictionModel(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(MachinePredictionModel, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_sensors = num_sensors\n",
    "        self.num_machines = num_machines\n",
    "\n",
    "        self.sensor_embedding = nn.Linear(num_machines, embed_size)\n",
    "        self.machine_embedding = nn.Linear(num_sensors, num_sensors)\n",
    "        self.attention = SingleHeadSelfAttention(embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, 1)  # For binary classification\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        num_machines, num_sensors = x.shape\n",
    "        assert num_machines == self.num_machines\n",
    "        assert num_sensors == self.num_sensors\n",
    "        \n",
    "        # Apply sensor embedding to each machine separately\n",
    "        machine_emb = self.machine_embedding(x) # 순서 바뀌면 안됨 원본 > machine, 원본 > layer 통과\n",
    "        x = self.sensor_embedding(x.T)  # Shape: (num_machines, embed_size)\n",
    " \n",
    "        \n",
    "        # Apply attention within each machine separately\n",
    "        # x = x.view(num_machines, self.num_sensors, self.embed_size)  # Shape: (num_machines, num_sensors, embed_size)\n",
    "        sensor_att, attention = self.attention(x, x, x, mask)  \n",
    "        x = torch.mm(machine_emb, sensor_att) # Shape: (num_machines, embed_size)\n",
    "\n",
    "        # Final output layer for each machine\n",
    "        out = self.fc_out(x).squeeze(-1)  # Shape: (num_machines)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddfb7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 32\n",
    "\n",
    "model = MachinePredictionModel(\n",
    "    embed_size=embed_size\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Assuming binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45fd42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.5815\n",
      "Epoch 2/100, Loss: 0.8088\n",
      "Epoch 3/100, Loss: 0.6187\n",
      "Epoch 4/100, Loss: 0.5833\n",
      "Epoch 5/100, Loss: 0.6460\n",
      "Epoch 6/100, Loss: 0.5619\n",
      "Epoch 7/100, Loss: 0.5380\n",
      "Epoch 8/100, Loss: 0.5510\n",
      "Epoch 9/100, Loss: 0.5567\n",
      "Epoch 10/100, Loss: 0.5458\n",
      "Epoch 11/100, Loss: 0.5258\n",
      "Epoch 12/100, Loss: 0.5076\n",
      "Epoch 13/100, Loss: 0.4988\n",
      "Epoch 14/100, Loss: 0.4994\n",
      "Epoch 15/100, Loss: 0.4998\n",
      "Epoch 16/100, Loss: 0.4920\n",
      "Epoch 17/100, Loss: 0.4791\n",
      "Epoch 18/100, Loss: 0.4671\n",
      "Epoch 19/100, Loss: 0.4586\n",
      "Epoch 20/100, Loss: 0.4529\n",
      "Epoch 21/100, Loss: 0.4481\n",
      "Epoch 22/100, Loss: 0.4428\n",
      "Epoch 23/100, Loss: 0.4365\n",
      "Epoch 24/100, Loss: 0.4292\n",
      "Epoch 25/100, Loss: 0.4215\n",
      "Epoch 26/100, Loss: 0.4142\n",
      "Epoch 27/100, Loss: 0.4077\n",
      "Epoch 28/100, Loss: 0.4021\n",
      "Epoch 29/100, Loss: 0.3971\n",
      "Epoch 30/100, Loss: 0.3921\n",
      "Epoch 31/100, Loss: 0.3865\n",
      "Epoch 32/100, Loss: 0.3806\n",
      "Epoch 33/100, Loss: 0.3746\n",
      "Epoch 34/100, Loss: 0.3691\n",
      "Epoch 35/100, Loss: 0.3641\n",
      "Epoch 36/100, Loss: 0.3597\n",
      "Epoch 37/100, Loss: 0.3555\n",
      "Epoch 38/100, Loss: 0.3514\n",
      "Epoch 39/100, Loss: 0.3472\n",
      "Epoch 40/100, Loss: 0.3429\n",
      "Epoch 41/100, Loss: 0.3387\n",
      "Epoch 42/100, Loss: 0.3348\n",
      "Epoch 43/100, Loss: 0.3311\n",
      "Epoch 44/100, Loss: 0.3277\n",
      "Epoch 45/100, Loss: 0.3246\n",
      "Epoch 46/100, Loss: 0.3216\n",
      "Epoch 47/100, Loss: 0.3187\n",
      "Epoch 48/100, Loss: 0.3157\n",
      "Epoch 49/100, Loss: 0.3129\n",
      "Epoch 50/100, Loss: 0.3102\n",
      "Epoch 51/100, Loss: 0.3076\n",
      "Epoch 52/100, Loss: 0.3053\n",
      "Epoch 53/100, Loss: 0.3030\n",
      "Epoch 54/100, Loss: 0.3009\n",
      "Epoch 55/100, Loss: 0.2988\n",
      "Epoch 56/100, Loss: 0.2967\n",
      "Epoch 57/100, Loss: 0.2947\n",
      "Epoch 58/100, Loss: 0.2928\n",
      "Epoch 59/100, Loss: 0.2909\n",
      "Epoch 60/100, Loss: 0.2892\n",
      "Epoch 61/100, Loss: 0.2875\n",
      "Epoch 62/100, Loss: 0.2859\n",
      "Epoch 63/100, Loss: 0.2844\n",
      "Epoch 64/100, Loss: 0.2828\n",
      "Epoch 65/100, Loss: 0.2814\n",
      "Epoch 66/100, Loss: 0.2799\n",
      "Epoch 67/100, Loss: 0.2785\n",
      "Epoch 68/100, Loss: 0.2772\n",
      "Epoch 69/100, Loss: 0.2759\n",
      "Epoch 70/100, Loss: 0.2747\n",
      "Epoch 71/100, Loss: 0.2734\n",
      "Epoch 72/100, Loss: 0.2722\n",
      "Epoch 73/100, Loss: 0.2711\n",
      "Epoch 74/100, Loss: 0.2699\n",
      "Epoch 75/100, Loss: 0.2688\n",
      "Epoch 76/100, Loss: 0.2677\n",
      "Epoch 77/100, Loss: 0.2667\n",
      "Epoch 78/100, Loss: 0.2657\n",
      "Epoch 79/100, Loss: 0.2647\n",
      "Epoch 80/100, Loss: 0.2637\n",
      "Epoch 81/100, Loss: 0.2627\n",
      "Epoch 82/100, Loss: 0.2618\n",
      "Epoch 83/100, Loss: 0.2608\n",
      "Epoch 84/100, Loss: 0.2599\n",
      "Epoch 85/100, Loss: 0.2590\n",
      "Epoch 86/100, Loss: 0.2582\n",
      "Epoch 87/100, Loss: 0.2573\n",
      "Epoch 88/100, Loss: 0.2564\n",
      "Epoch 89/100, Loss: 0.2556\n",
      "Epoch 90/100, Loss: 0.2548\n",
      "Epoch 91/100, Loss: 0.2540\n",
      "Epoch 92/100, Loss: 0.2532\n",
      "Epoch 93/100, Loss: 0.2524\n",
      "Epoch 94/100, Loss: 0.2517\n",
      "Epoch 95/100, Loss: 0.2509\n",
      "Epoch 96/100, Loss: 0.2502\n",
      "Epoch 97/100, Loss: 0.2495\n",
      "Epoch 98/100, Loss: 0.2487\n",
      "Epoch 99/100, Loss: 0.2480\n",
      "Epoch 100/100, Loss: 0.2474\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 22\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_test_tensor)\n\u001b[0;32m     24\u001b[0m     val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_test_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[34], line 15\u001b[0m, in \u001b[0;36mMachinePredictionModel.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     14\u001b[0m     num_machines, num_sensors \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_machines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_machines\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m num_sensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_sensors\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Apply sensor embedding to each machine separately\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# 검증 루프\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    loss = criterion(outputs, y_test_tensor)\n",
    "    val_loss += loss.item() * X_test_tensor.size(0)\n",
    "    predicted = (outputs > 0.5).float()\n",
    "    total += y_test_tensor.size(0)\n",
    "    correct += (predicted == y_test_tensor).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "fold_results.append((val_loss, accuracy))\n",
    "\n",
    "# 교차 검증 결과 출력\n",
    "fold_results = np.array(fold_results)\n",
    "mean_loss = fold_results[:, 0].mean()\n",
    "mean_accuracy = fold_results[:, 1].mean()\n",
    "print(f'5-Fold Cross-Validation Results:')\n",
    "print(f'Average Validation Loss: {mean_loss:.4f}')\n",
    "print(f'Average Validation Accuracy: {mean_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fbecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
