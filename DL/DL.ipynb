{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6eea7e",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d39d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BinaryClassificationModel, Classifier\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset,Subset,random_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8c8c6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024a6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data.csv')\n",
    "\n",
    "data_x = data.iloc[:,:-1]\n",
    "data_y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9118886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# 데이터 표준화\n",
    "standardized_data_x = scaler.fit_transform(data_x)\n",
    "data_s_x = pd.DataFrame(standardized_data_x, columns = data_x.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_s_x, data_y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21a748",
   "metadata": {},
   "source": [
    "### Hyperparameter 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c461a01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor로 변환\n",
    "X_train, X_test, y_train, y_test = train_test_split(standardized_data_x, data_y, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train,dtype = torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_list(),dtype = torch.float32).unsqueeze(1)  # y를 (1000, 1) 크기로 변환\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test,dtype = torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_list(),dtype = torch.float32).unsqueeze(1)  # y를 (1000, 1) 크기로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc53ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n",
      "C:\\Users\\yy950\\DataScience\\model.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  outputs = self.model(torch.tensor(X).float())\n",
      "C:\\Users\\yy950\\DataScience\\model.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset = TensorDataset(torch.tensor(X).float(), torch.tensor(y).float().unsqueeze(1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'num_epochs': 30, 'lr': 0.001, 'hidden_size2': 32, 'hidden_size1': 64, 'batch_size': 16}\n",
      "Best cross-validation accuracy: 0.5801\n"
     ]
    }
   ],
   "source": [
    "parameter = {\n",
    "    'num_epochs' : [20,30,50],\n",
    "    'lr' : [0.01,0.001,0.05],\n",
    "    'batch_size' : [16,32,64],\n",
    "    'hidden_size1': [32, 64],\n",
    "    'hidden_size2': [16, 32]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=Classifier(), param_distributions=parameter,n_iter = 10, cv=5, scoring='accuracy', verbose=1)\n",
    "# 그리드 서치 실행\n",
    "random_search.fit(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.4f}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea8f8d6",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29535cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/30, Loss: 0.6028\n",
      "Epoch 2/30, Loss: 0.3989\n",
      "Epoch 3/30, Loss: 0.2583\n",
      "Epoch 4/30, Loss: 0.2213\n",
      "Epoch 5/30, Loss: 0.2077\n",
      "Epoch 6/30, Loss: 0.1980\n",
      "Epoch 7/30, Loss: 0.1915\n",
      "Epoch 8/30, Loss: 0.1858\n",
      "Epoch 9/30, Loss: 0.1822\n",
      "Epoch 10/30, Loss: 0.1774\n",
      "Epoch 11/30, Loss: 0.1719\n",
      "Epoch 12/30, Loss: 0.1680\n",
      "Epoch 13/30, Loss: 0.1643\n",
      "Epoch 14/30, Loss: 0.1594\n",
      "Epoch 15/30, Loss: 0.1541\n",
      "Epoch 16/30, Loss: 0.1560\n",
      "Epoch 17/30, Loss: 0.1474\n",
      "Epoch 18/30, Loss: 0.1438\n",
      "Epoch 19/30, Loss: 0.1461\n",
      "Epoch 20/30, Loss: 0.1410\n",
      "Epoch 21/30, Loss: 0.1402\n",
      "Epoch 22/30, Loss: 0.1342\n",
      "Epoch 23/30, Loss: 0.1293\n",
      "Epoch 24/30, Loss: 0.1295\n",
      "Epoch 25/30, Loss: 0.1282\n",
      "Epoch 26/30, Loss: 0.1278\n",
      "Epoch 27/30, Loss: 0.1234\n",
      "Epoch 28/30, Loss: 0.1201\n",
      "Epoch 29/30, Loss: 0.1211\n",
      "Epoch 30/30, Loss: 0.1168\n",
      "Validation Loss: 0.2589, Accuracy: 0.9139\n",
      "Fold 2\n",
      "Epoch 1/30, Loss: 0.5998\n",
      "Epoch 2/30, Loss: 0.3974\n",
      "Epoch 3/30, Loss: 0.2867\n",
      "Epoch 4/30, Loss: 0.2429\n",
      "Epoch 5/30, Loss: 0.2262\n",
      "Epoch 6/30, Loss: 0.2149\n",
      "Epoch 7/30, Loss: 0.2104\n",
      "Epoch 8/30, Loss: 0.2026\n",
      "Epoch 9/30, Loss: 0.2012\n",
      "Epoch 10/30, Loss: 0.1940\n",
      "Epoch 11/30, Loss: 0.1925\n",
      "Epoch 12/30, Loss: 0.1865\n",
      "Epoch 13/30, Loss: 0.1834\n",
      "Epoch 14/30, Loss: 0.1787\n",
      "Epoch 15/30, Loss: 0.1736\n",
      "Epoch 16/30, Loss: 0.1719\n",
      "Epoch 17/30, Loss: 0.1699\n",
      "Epoch 18/30, Loss: 0.1657\n",
      "Epoch 19/30, Loss: 0.1649\n",
      "Epoch 20/30, Loss: 0.1621\n",
      "Epoch 21/30, Loss: 0.1600\n",
      "Epoch 22/30, Loss: 0.1573\n",
      "Epoch 23/30, Loss: 0.1541\n",
      "Epoch 24/30, Loss: 0.1521\n",
      "Epoch 25/30, Loss: 0.1503\n",
      "Epoch 26/30, Loss: 0.1486\n",
      "Epoch 27/30, Loss: 0.1468\n",
      "Epoch 28/30, Loss: 0.1424\n",
      "Epoch 29/30, Loss: 0.1436\n",
      "Epoch 30/30, Loss: 0.1429\n",
      "Validation Loss: 0.1462, Accuracy: 0.9338\n",
      "Fold 3\n",
      "Epoch 1/30, Loss: 0.6264\n",
      "Epoch 2/30, Loss: 0.4095\n",
      "Epoch 3/30, Loss: 0.2662\n",
      "Epoch 4/30, Loss: 0.2272\n",
      "Epoch 5/30, Loss: 0.2128\n",
      "Epoch 6/30, Loss: 0.2048\n",
      "Epoch 7/30, Loss: 0.1994\n",
      "Epoch 8/30, Loss: 0.1925\n",
      "Epoch 9/30, Loss: 0.1887\n",
      "Epoch 10/30, Loss: 0.1830\n",
      "Epoch 11/30, Loss: 0.1787\n",
      "Epoch 12/30, Loss: 0.1737\n",
      "Epoch 13/30, Loss: 0.1698\n",
      "Epoch 14/30, Loss: 0.1661\n",
      "Epoch 15/30, Loss: 0.1626\n",
      "Epoch 16/30, Loss: 0.1602\n",
      "Epoch 17/30, Loss: 0.1558\n",
      "Epoch 18/30, Loss: 0.1523\n",
      "Epoch 19/30, Loss: 0.1509\n",
      "Epoch 20/30, Loss: 0.1466\n",
      "Epoch 21/30, Loss: 0.1429\n",
      "Epoch 22/30, Loss: 0.1420\n",
      "Epoch 23/30, Loss: 0.1387\n",
      "Epoch 24/30, Loss: 0.1347\n",
      "Epoch 25/30, Loss: 0.1341\n",
      "Epoch 26/30, Loss: 0.1309\n",
      "Epoch 27/30, Loss: 0.1254\n",
      "Epoch 28/30, Loss: 0.1247\n",
      "Epoch 29/30, Loss: 0.1221\n",
      "Epoch 30/30, Loss: 0.1196\n",
      "Validation Loss: 0.2219, Accuracy: 0.9139\n",
      "Fold 4\n",
      "Epoch 1/30, Loss: 0.6078\n",
      "Epoch 2/30, Loss: 0.4213\n",
      "Epoch 3/30, Loss: 0.2781\n",
      "Epoch 4/30, Loss: 0.2292\n",
      "Epoch 5/30, Loss: 0.2120\n",
      "Epoch 6/30, Loss: 0.2042\n",
      "Epoch 7/30, Loss: 0.1999\n",
      "Epoch 8/30, Loss: 0.1922\n",
      "Epoch 9/30, Loss: 0.1895\n",
      "Epoch 10/30, Loss: 0.1837\n",
      "Epoch 11/30, Loss: 0.1795\n",
      "Epoch 12/30, Loss: 0.1762\n",
      "Epoch 13/30, Loss: 0.1738\n",
      "Epoch 14/30, Loss: 0.1708\n",
      "Epoch 15/30, Loss: 0.1676\n",
      "Epoch 16/30, Loss: 0.1619\n",
      "Epoch 17/30, Loss: 0.1591\n",
      "Epoch 18/30, Loss: 0.1553\n",
      "Epoch 19/30, Loss: 0.1539\n",
      "Epoch 20/30, Loss: 0.1503\n",
      "Epoch 21/30, Loss: 0.1481\n",
      "Epoch 22/30, Loss: 0.1465\n",
      "Epoch 23/30, Loss: 0.1424\n",
      "Epoch 24/30, Loss: 0.1386\n",
      "Epoch 25/30, Loss: 0.1364\n",
      "Epoch 26/30, Loss: 0.1364\n",
      "Epoch 27/30, Loss: 0.1328\n",
      "Epoch 28/30, Loss: 0.1308\n",
      "Epoch 29/30, Loss: 0.1296\n",
      "Epoch 30/30, Loss: 0.1266\n",
      "Validation Loss: 0.2040, Accuracy: 0.9007\n",
      "Fold 5\n",
      "Epoch 1/30, Loss: 0.5920\n",
      "Epoch 2/30, Loss: 0.3688\n",
      "Epoch 3/30, Loss: 0.2353\n",
      "Epoch 4/30, Loss: 0.2001\n",
      "Epoch 5/30, Loss: 0.1913\n",
      "Epoch 6/30, Loss: 0.1818\n",
      "Epoch 7/30, Loss: 0.1797\n",
      "Epoch 8/30, Loss: 0.1720\n",
      "Epoch 9/30, Loss: 0.1683\n",
      "Epoch 10/30, Loss: 0.1674\n",
      "Epoch 11/30, Loss: 0.1612\n",
      "Epoch 12/30, Loss: 0.1594\n",
      "Epoch 13/30, Loss: 0.1559\n",
      "Epoch 14/30, Loss: 0.1558\n",
      "Epoch 15/30, Loss: 0.1510\n",
      "Epoch 16/30, Loss: 0.1480\n",
      "Epoch 17/30, Loss: 0.1495\n",
      "Epoch 18/30, Loss: 0.1418\n",
      "Epoch 19/30, Loss: 0.1424\n",
      "Epoch 20/30, Loss: 0.1401\n",
      "Epoch 21/30, Loss: 0.1365\n",
      "Epoch 22/30, Loss: 0.1379\n",
      "Epoch 23/30, Loss: 0.1342\n",
      "Epoch 24/30, Loss: 0.1323\n",
      "Epoch 25/30, Loss: 0.1323\n",
      "Epoch 26/30, Loss: 0.1321\n",
      "Epoch 27/30, Loss: 0.1269\n",
      "Epoch 28/30, Loss: 0.1262\n",
      "Epoch 29/30, Loss: 0.1227\n",
      "Epoch 30/30, Loss: 0.1219\n",
      "Validation Loss: 0.2967, Accuracy: 0.9007\n",
      "5-Fold Cross-Validation Results:\n",
      "Average Validation Loss: 0.2256\n",
      "Average Validation Accuracy: 0.9126\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'num_epochs': 30,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 16,\n",
    "    'hidden_size1': 64,\n",
    "    'hidden_size2': 32\n",
    "}\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f'Fold {fold+1}')\n",
    "    \n",
    "    # 데이터셋 준비\n",
    "    train_subset = Subset(TensorDataset(X_train_tensor, y_train_tensor), train_idx.tolist())\n",
    "    val_subset = Subset(TensorDataset(X_train_tensor, y_train_tensor), val_idx.tolist())\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # 모델, 손실 함수, 옵티마이저 정의\n",
    "    model = BinaryClassificationModel()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    # 학습 루프\n",
    "    for epoch in range(params['num_epochs']):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_subset)\n",
    "        print(f'Epoch {epoch+1}/{params[\"num_epochs\"]}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_subset)\n",
    "    accuracy = correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    fold_results.append((val_loss, accuracy))\n",
    "\n",
    "# 교차 검증 결과 출력\n",
    "fold_results = np.array(fold_results)\n",
    "mean_loss = fold_results[:, 0].mean()\n",
    "mean_accuracy = fold_results[:, 1].mean()\n",
    "print(f'5-Fold Cross-Validation Results:')\n",
    "print(f'Average Validation Loss: {mean_loss:.4f}')\n",
    "print(f'Average Validation Accuracy: {mean_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d66839",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3258c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5S0lEQVR4nO3deXwV9b3/8ffJcrJJQtlCAiEsGkWtAklZf9RCWQQvVFokFpSlgERQhKhcKa0sWlOtIqAsIptYBKyC1SsucWPVCwlJQeEqlZSwJMUgJEAgIcn39wcPTjkm4JnkJCeZvJ6Px3k8PN8zM+czk5h58/1+Z8ZhjDECAACwCT9fFwAAAOBNhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArAb4uoKaVlZXp2LFjatCggRwOh6/LAQAAHjDG6PTp04qOjpaf39X7ZupduDl27JhiYmJ8XQYAAKiEw4cPq2XLllddpt6FmwYNGki6eHDCw8N9XA0AAPBEQUGBYmJiXOfxq6l34ebSUFR4eDjhBgCAOsaTKSVMKAYAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALbi03CzZcsWDRo0SNHR0XI4HHrrrbd+dJ3NmzcrPj5ewcHBatu2rZYsWVL9hQIAgDrDp+Hm7NmzuvXWW/Xiiy96tHxWVpYGDhyonj17KiMjQ7///e81efJkvfnmm9VcKQAAqCt8+uDMAQMGaMCAAR4vv2TJErVq1Urz5s2TJLVv315paWl69tln9Zvf/KaaqgQA1AfGGJ27UOrrMmwjJNDfo4dcVoc69VTwzz//XP369XNr69+/v5YvX64LFy4oMDCw3DpFRUUqKipyvS8oKKj2OgEAdYsxRkOXfK70Qyd9XYpt7JvTX6FO38SMOjWhODc3V5GRkW5tkZGRKikpUV5eXoXrpKSkKCIiwvWKiYmpiVIBAHXIuQulBBsbqVM9N5LKdXEZYypsv2T69OlKTk52vS8oKCDgAACuKO0PfRTq9Pd1GXVeSKDvjmGdCjfNmzdXbm6uW9vx48cVEBCgxo0bV7hOUFCQgoKCaqI8AB5gXgNqo8Li//xOhjr9fTacAu+oUz+9bt266Z133nFr+/DDD5WQkFDhfBsAtQvzGgDUBJ/OuTlz5owyMzOVmZkp6eKl3pmZmcrOzpZ0cUhp5MiRruWTkpJ06NAhJScna//+/VqxYoWWL1+uRx55xBflA7CIeQ2o7RJif+LT4RR4h097btLS0tSrVy/X+0tzY0aNGqVVq1YpJyfHFXQkqU2bNtq0aZOmTp2qhQsXKjo6WgsWLOAycKAOYl4DaiNfXr4M73GYSzNy64mCggJFREQoPz9f4eHhvi4HqJS6Om+lsLhUCU9+JMm3l4kCqHusnL/5ywLUMcxbAYCrq1P3uQFgj3krzGsAUJ3ouQHqsLo6b4V5DQCqE+EGqEbVMTeG+3EAwNXxVxGoJsyNAQDfYM4NUE2qe24M81YAoGL03AA1oDrmxjBvBQAqRrgBagBzYwCg5jAsBQAAbIVwAwAAbIVwAwAAbIVJAKj16vJzlAAANY9wg1qNe8UAAKxiWAq1Gs9RAgBYRc8N6gyeowQA8AThBnUG94oBAHiCYSkAAGArhBsAAGArhBsAAGArTGBArXP5fW24VwwAwCrCDWoV7msDAKgqhqVQq1zpvjbcKwYA4Cl6blBrXX5fG+4VAwDwFOEGHquJZzxdPseG+9oAACqDMwc8wlwYAEBdwZwbeKSmn/HEHBsAQGXRcwPLauIZT8yxAQBUFuEGFfrh/BrmwgAA6grOUCiH+TUAgLqMOTco52rza5gLAwCo7ei5wVX9cH4Nc2EAALUd4aaesHKPGubXAADqMs5a9QBzaAAA9QlzbuqByt6jhvk1AIC6iJ6besbKPWqYXwMAqIsINzbwY/NpmEMDAKhPOMvVccynAQDAHXNu6jgr82mYQwMAqA/oubGRH5tPwxwaAEB9QLixEebTAADAsBQAALAZwg0AALAVwg0AALAVJmjUclbuYQMAAAg3tRr3sAEAwDqGpWox7mEDAIB19NzUEdzDBgAAzxBu6gjuYQMAgGcYlgIAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALbCjVNqmcufJcVzowAAsI5wU4vwLCkAAKrO58NSixYtUps2bRQcHKz4+Hht3br1qsuvWbNGt956q0JDQxUVFaUxY8boxIkTNVRt9brSs6R4bhQAAJ7zac/N+vXrNWXKFC1atEg9evTQSy+9pAEDBmjfvn1q1apVueW3bdumkSNH6vnnn9egQYN09OhRJSUlady4cdq4caMP9qD6XP4sKZ4bBQCA53zaczN37lyNHTtW48aNU/v27TVv3jzFxMRo8eLFFS7/xRdfqHXr1po8ebLatGmj//f//p8mTJigtLS0K35HUVGRCgoK3F51waVnSYU6Awg2AABY4LNwU1xcrPT0dPXr18+tvV+/ftqxY0eF63Tv3l1HjhzRpk2bZIzRv//9b73xxhu64447rvg9KSkpioiIcL1iYmK8uh8AAKB28Vm4ycvLU2lpqSIjI93aIyMjlZubW+E63bt315o1a5SYmCin06nmzZurYcOGeuGFF674PdOnT1d+fr7rdfjwYa/uBwAAqF18PqH4h0MuxpgrDsPs27dPkydP1uOPP6709HS9//77ysrKUlJS0hW3HxQUpPDwcLcXAACwL59NKG7SpIn8/f3L9dIcP368XG/OJSkpKerRo4ceffRRSdItt9yisLAw9ezZU08++aSioqKqvW4AAFC7+aznxul0Kj4+XqmpqW7tqamp6t69e4XrFBYWys/PvWR//4tXFBljqqdQAABQp/h0WCo5OVnLli3TihUrtH//fk2dOlXZ2dmuYabp06dr5MiRruUHDRqkDRs2aPHixTp48KC2b9+uyZMnq3PnzoqOjvbVbgAAgFrEp/e5SUxM1IkTJzRnzhzl5OTo5ptv1qZNmxQbGytJysnJUXZ2tmv50aNH6/Tp03rxxRf18MMPq2HDhurdu7eefvppX+0CAACoZRymno3nFBQUKCIiQvn5+bVicvEPnyWV8ORHkqR9c/or1MnTMQAAkKydvzl7+hDPkgIAwPt8fil4fcazpAAA8D56bmoJniUFAIB3EG5qiUvPkgIAAFXDsBQAALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALAVwg0AALCVAF8XUN8YY3TuQqkkqbC41MfVAABgP4SbGmSM0dAlnyv90ElflwIAgG0xLFWDzl0orTDYJMT+RCGB/j6oCAAA+6HnxkfS/tBHoc6LgSYk0F8Oh8PHFQEAYA+EGx8Jdfor1MnhBwDA2xiWAgAAtkK4AQAAtkK4AQAAtkK4AQAAtkK4AQAAtkK4AQAAtkK4AQAAtkK4AQAAtkK4AQAAtlKpcFNSUqKPPvpIL730kk6fPi1JOnbsmM6cOePV4gAAAKyyfP//Q4cO6fbbb1d2draKiorUt29fNWjQQM8884zOnz+vJUuWVEedAAAAHrHcc/PQQw8pISFBJ0+eVEhIiKt9yJAh+vjjj71aXF1njFFhccllr1JflwQAgO1Z7rnZtm2btm/fLqfT6dYeGxuro0ePeq2wus4Yo6FLPlf6oZO+LgUAgHrFcs9NWVmZSkvL90AcOXJEDRo08EpRdnDuQukVg01C7E8UEuhfwxUBAFA/WO656du3r+bNm6elS5dKkhwOh86cOaOZM2dq4MCBXi/QDtL+0Eehzv+EmZBAfzkcDh9WBACAfVkON88//7x69eqlG2+8UefPn9fw4cN14MABNWnSRGvXrq2OGuu8UKe/Qp2WDzUAAKgEy2fc6OhoZWZmat26dUpPT1dZWZnGjh2rESNGuE0wBgAA8AXL4WbLli3q3r27xowZozFjxrjaS0pKtGXLFv385z/3aoEAAABWWJ5Q3KtXL33//ffl2vPz89WrVy+vFAUAAFBZlsONMabCybAnTpxQWFiYV4oCAACoLI+HpX79619Lunh11OjRoxUUFOT6rLS0VHv27FH37t29XyEAAIAFHoebiIgISRd7bho0aOA2edjpdKpr164aP3689ysEAACwwONws3LlSklS69at9cgjjzAEBQAAaiXLV0vNnDmzOuoAAADwikrdWe6NN97Q66+/ruzsbBUXF7t9tnv3bq8UBgAAUBmWr5ZasGCBxowZo2bNmikjI0OdO3dW48aNdfDgQQ0YMKA6agQAAPCY5XCzaNEiLV26VC+++KKcTqemTZum1NRUTZ48Wfn5+dVRIwAAgMcsh5vs7GzXJd8hISE6ffq0JOnee+/l2VIAAMDnLIeb5s2b68SJE5Kk2NhYffHFF5KkrKwsGWO8Wx0AAIBFlsNN79699c4770iSxo4dq6lTp6pv375KTEzUkCFDvF4gAACAFZavllq6dKnKysokSUlJSWrUqJG2bdumQYMGKSkpyesFAgAAWGE53Pj5+cnP7z8dPsOGDdOwYcMkSUePHlWLFi28Vx0AAIBFloelKpKbm6sHH3xQ1157reV1Fy1apDZt2ig4OFjx8fHaunXrVZcvKirSjBkzFBsbq6CgILVr104rVqyobOkAAMBmPA43p06d0ogRI9S0aVNFR0drwYIFKisr0+OPP662bdvqiy++sBwy1q9frylTpmjGjBnKyMhQz549NWDAAGVnZ19xnWHDhunjjz/W8uXL9fXXX2vt2rW64YYbLH0vAACwL4fx8BKniRMn6p133lFiYqLef/997d+/X/3799f58+c1c+ZM3XbbbZa/vEuXLurUqZMWL17samvfvr3uvPNOpaSklFv+/fff1913362DBw+qUaNGHn1HUVGRioqKXO8LCgoUExOj/Px8hYeHW67ZU4XFJbrx8Q8kSfvm9Feos1I3gwYAALp4/o6IiPDo/O1xz827776rlStX6tlnn9Xbb78tY4zi4uL0ySefVCrYFBcXKz09Xf369XNr79evn3bs2FHhOm+//bYSEhL0zDPPqEWLFoqLi9Mjjzyic+fOXfF7UlJSFBER4XrFxMRYrhUAANQdHncnHDt2TDfeeKMkqW3btgoODta4ceMq/cV5eXkqLS1VZGSkW3tkZKRyc3MrXOfgwYPatm2bgoODtXHjRuXl5WnixIn6/vvvrzgkNn36dCUnJ7veX+q5AQAA9uRxuCkrK1NgYKDrvb+/v8LCwqpcgMPhcHtvjCnXdnkNDodDa9asUUREhCRp7ty5Gjp0qBYuXKiQkJBy6wQFBSkoKKjKdQIAgLrB43BjjNHo0aNdQeH8+fNKSkoqF3A2bNjg0faaNGkif3//cr00x48fL9ebc0lUVJRatGjhCjbSxTk6xhgdOXJE1113nae7AwAAbMrjOTejRo1Ss2bNXHNX7rnnHkVHR7vNZ7k8dPwYp9Op+Ph4paamurWnpqa6nl31Qz169NCxY8d05swZV9s333wjPz8/tWzZ0uPvBgAA9uVxz83KlSu9/uXJycm69957lZCQoG7dumnp0qXKzs523el4+vTpOnr0qFavXi1JGj58uJ544gmNGTNGs2fPVl5enh599FH97ne/q3BICgAA1D8+vT45MTFRJ06c0Jw5c5STk6Obb75ZmzZtUmxsrCQpJyfH7Z4311xzjVJTU/Xggw8qISFBjRs31rBhw/Tkk0/6ahcAAEAt4/F9buzCynXyVcF9bgAA8J5quc8NAABAXUC4AQAAtkK4AQAAtlKpcPPqq6+qR48eio6O1qFDhyRJ8+bN09///nevFgcAAGCV5XCzePFiJScna+DAgTp16pRKS0slSQ0bNtS8efO8XR8AAIAllsPNCy+8oJdfflkzZsyQv7+/qz0hIUF79+71anEAAABWWQ43WVlZ6tixY7n2oKAgnT171itFAQAAVJblcNOmTRtlZmaWa3/vvfdcTw0HAADwFct3lnv00Uc1adIknT9/XsYY7dy5U2vXrlVKSoqWLVtWHTUCAAB4zHK4GTNmjEpKSjRt2jQVFhZq+PDhatGihebPn6+77767OmoEAADwWKWeCTB+/HiNHz9eeXl5KisrU7NmzbxdFwAAQKVYnnMze/Zsffvtt5KkJk2aEGwAAECtYjncvPnmm4qLi1PXrl314osv6rvvvquOugAAACrFcrjZs2eP9uzZo969e2vu3Llq0aKFBg4cqNdee02FhYXVUSMAAIDHKvX4hZtuuklPPfWUDh48qE8//VRt2rTRlClT1Lx5c2/XBwAAYEmVH5wZFhamkJAQOZ1OXbhwwRs1AQAAVFqlwk1WVpb+9Kc/6cYbb1RCQoJ2796tWbNmKTc319v1AQAAWGL5UvBu3bpp586d+ulPf6oxY8a47nMDAABQG1gON7169dKyZct00003VUc9AAAAVWI53Dz11FPVUQcAAIBXeBRukpOT9cQTTygsLEzJyclXXXbu3LleKawuMsbo3IVSSVJhcamPqwEAoH7yKNxkZGS4roTKyMio1oLqKmOMhi75XOmHTvq6FAAA6jWPws2nn35a4X/jP85dKK0w2CTE/kQhgf4+qAgAgPrJ8pyb3/3ud5o/f74aNGjg1n727Fk9+OCDWrFihdeKq6vS/tBHoc6LgSYk0F8Oh8PHFQEAUH9Yvs/NK6+8onPnzpVrP3funFavXu2Vouq6UKe/Qp0BCnUGEGwAAKhhHvfcFBQUyBgjY4xOnz6t4OBg12elpaXatGkTTwgHAAA+53G4adiwoRwOhxwOh+Li4sp97nA4NHv2bK8WBwAAYJXH4ebTTz+VMUa9e/fWm2++qUaNGrk+czqdio2NVXR0dLUUCQAA4CmPw81tt90m6eJzpVq1asVcEgAAUCt5FG727Nmjm2++WX5+fsrPz9fevXuvuOwtt9ziteIAAACs8ijcdOjQQbm5uWrWrJk6dOggh8MhY0y55RwOh0pLuTMvAADwHY/CTVZWlpo2ber6bwAAgNrKo3ATGxtb4X8DAADUNpW6id+7777rej9t2jQ1bNhQ3bt316FDh7xaHAAAgFWWw81TTz2lkJAQSdLnn3+uF198Uc8884yaNGmiqVOner1AAAAAKyw/W+rw4cO69tprJUlvvfWWhg4dqvvuu089evTQL37xC2/XBwAAYInlnptrrrlGJ06ckCR9+OGH6tOnjyQpODi4wmdOAQAA1CTLPTd9+/bVuHHj1LFjR33zzTe64447JElfffWVWrdu7e36AAAALLHcc7Nw4UJ169ZN3333nd588001btxYkpSenq7f/va3Xi8QAADACss9Nw0bNtSLL75Yrp2HZgIAgNrAcriRpFOnTmn58uXav3+/HA6H2rdvr7FjxyoiIsLb9QEAAFhieVgqLS1N7dq10/PPP6/vv/9eeXl5ev7559WuXTvt3r27OmoEAADwmOWem6lTp2rw4MF6+eWXFRBwcfWSkhKNGzdOU6ZM0ZYtW7xeJAAAgKcsh5u0tDS3YCNJAQEBmjZtmhISErxaHAAAgFWWh6XCw8OVnZ1drv3w4cNq0KCBV4oCAACoLMvhJjExUWPHjtX69et1+PBhHTlyROvWrdO4ceO4FBwAAPic5WGpZ599Vg6HQyNHjlRJSYkkKTAwUPfff7/+/Oc/e71AAAAAKyyHG6fTqfnz5yslJUXffvutjDG69tprFRoaWh31AQAAWOLxsFRhYaEmTZqkFi1aqFmzZho3bpyioqJ0yy23EGwAAECt4XG4mTlzplatWqU77rhDd999t1JTU3X//fdXZ20AAACWeTwstWHDBi1fvlx33323JOmee+5Rjx49VFpaKn9//2orEAAAwAqPe24OHz6snj17ut537txZAQEBOnbsWLUUBgAAUBkeh5vS0lI5nU63toCAANcVUwAAALWBx8NSxhiNHj1aQUFBrrbz588rKSlJYWFhrrYNGzZ4t0IAAAALPA43o0aNKtd2zz33eLUYAACAqvI43KxcubI66wAAAPAKy49f8LZFixapTZs2Cg4OVnx8vLZu3erRetu3b1dAQIA6dOhQvQUCAIA6xafhZv369ZoyZYpmzJihjIwM9ezZUwMGDKjwwZyXy8/P18iRI/XLX/6yhioFAAB1hU/Dzdy5czV27FiNGzdO7du317x58xQTE6PFixdfdb0JEyZo+PDh6tatWw1VCgAA6gqfhZvi4mKlp6erX79+bu39+vXTjh07rrjeypUr9e2332rmzJkefU9RUZEKCgrcXgAAwL58Fm7y8vJUWlqqyMhIt/bIyEjl5uZWuM6BAwf02GOPac2aNQoI8GwudEpKiiIiIlyvmJiYKtcOAABqr0qFm1dffVU9evRQdHS0Dh06JEmaN2+e/v73v1velsPhcHtvjCnXJl28ieDw4cM1e/ZsxcXFebz96dOnKz8/3/U6fPiw5RoBAEDdYTncLF68WMnJyRo4cKBOnTql0tJSSVLDhg01b948j7fTpEkT+fv7l+ulOX78eLneHEk6ffq00tLS9MADDyggIEABAQGaM2eO/vGPfyggIECffPJJhd8TFBSk8PBwtxcAALAvy+HmhRde0Msvv6wZM2a4PTAzISFBe/fu9Xg7TqdT8fHxSk1NdWtPTU1V9+7dyy0fHh6uvXv3KjMz0/VKSkrS9ddfr8zMTHXp0sXqrgAAABvy+CZ+l2RlZaljx47l2oOCgnT27FlL20pOTta9996rhIQEdevWTUuXLlV2draSkpIkXRxSOnr0qFavXi0/Pz/dfPPNbus3a9ZMwcHB5doBAED9ZTnctGnTRpmZmYqNjXVrf++993TjjTda2lZiYqJOnDihOXPmKCcnRzfffLM2bdrk2nZOTs6P3vMGAADgcpbDzaOPPqpJkybp/PnzMsZo586dWrt2rVJSUrRs2TLLBUycOFETJ06s8LNVq1Zddd1Zs2Zp1qxZlr8TAADYl+VwM2bMGJWUlGjatGkqLCzU8OHD1aJFC82fP1933313ddQIAADgMcvhRpLGjx+v8ePHKy8vT2VlZWrWrJm36wIAAKiUSoWbS5o0aeKtOgAAALyiUhOKK7rJ3iUHDx6sUkEAAABVYTncTJkyxe39hQsXlJGRoffff1+PPvqot+oCAACoFMvh5qGHHqqwfeHChUpLS6tyQQAAAFXhtQdnDhgwQG+++aa3NgcAAFApXgs3b7zxhho1auStzQEAAFSK5WGpjh07uk0oNsYoNzdX3333nRYtWuTV4gAAAKyyHG7uvPNOt/d+fn5q2rSpfvGLX+iGG27wVl0AAACVYinclJSUqHXr1urfv7+aN29eXTUBAABUmqU5NwEBAbr//vtVVFRUXfUAAABUieUJxV26dFFGRkZ11AIAAFBllufcTJw4UQ8//LCOHDmi+Ph4hYWFuX1+yy23eK04AAAAqzwON7/73e80b948JSYmSpImT57s+szhcMgYI4fDodLSUu9XCQAA4CGPw80rr7yiP//5z8rKyqrOegAAAKrE43BjjJEkxcbGVlsxAAAAVWVpQvHVngYOAABQG1iaUBwXF/ejAef777+vUkEAAABVYSnczJ49WxEREdVVCwAAQJVZCjd33323mjVrVl21AAAAVJnHc26YbwMAAOoCj8PNpaulAAAAajOPh6XKysqqsw4AAACvsPxsKQAAgNqMcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGzF5+Fm0aJFatOmjYKDgxUfH6+tW7decdkNGzaob9++atq0qcLDw9WtWzd98MEHNVgtAACo7XwabtavX68pU6ZoxowZysjIUM+ePTVgwABlZ2dXuPyWLVvUt29fbdq0Senp6erVq5cGDRqkjIyMGq4cAADUVg5jjPHVl3fp0kWdOnXS4sWLXW3t27fXnXfeqZSUFI+2cdNNNykxMVGPP/64R8sXFBQoIiJC+fn5Cg8Pr1TdFSksLtGNj1/sRdo3p79CnQFe2zYAAPWdlfO3z3puiouLlZ6ern79+rm19+vXTzt27PBoG2VlZTp9+rQaNWp0xWWKiopUUFDg9gIAAPbls3CTl5en0tJSRUZGurVHRkYqNzfXo20899xzOnv2rIYNG3bFZVJSUhQREeF6xcTEVKluAABQu/l8QrHD4XB7b4wp11aRtWvXatasWVq/fr2aNWt2xeWmT5+u/Px81+vw4cNVrhkAANRePpsY0qRJE/n7+5frpTl+/Hi53pwfWr9+vcaOHau//e1v6tOnz1WXDQoKUlBQUJXrBQAAdYPPem6cTqfi4+OVmprq1p6amqru3btfcb21a9dq9OjReu2113THHXdUd5kAAKCO8eklPcnJybr33nuVkJCgbt26aenSpcrOzlZSUpKki0NKR48e1erVqyVdDDYjR47U/Pnz1bVrV1evT0hIiCIiIny2HwAAoPbwabhJTEzUiRMnNGfOHOXk5Ojmm2/Wpk2bFBsbK0nKyclxu+fNSy+9pJKSEk2aNEmTJk1ytY8aNUqrVq2q6fIBAEAt5NP73PgC97kBAKDuqRP3uQEAAKgOhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArhBsAAGArAb4uAAAASTLGqKSkRKWlpb4uBT4SGBgof3//Km+HcAMA8Lni4mLl5OSosLDQ16XAhxwOh1q2bKlrrrmmStsh3AAAfKqsrExZWVny9/dXdHS0nE6nHA6Hr8tCDTPG6LvvvtORI0d03XXXVakHh3ADAPCp4uJilZWVKSYmRqGhob4uBz7UtGlT/etf/9KFCxeqFG6YUAwAqBX8/Dgl1Xfe6rHjNwkAANgK4QYAANgK4QYAANgK4QYAgCrasWOH/P39dfvtt5f77LPPPpPD4dCpU6fKfdahQwfNmjXLrS0jI0N33XWXIiMjFRwcrLi4OI0fP17ffPNNNVV/0aJFi9SmTRsFBwcrPj5eW7du/dF1Fi5cqPbt2yskJETXX3+9Vq9e7fb5qlWr5HA4yr3Onz9fXbshiXADAECVrVixQg8++KC2bdum7OzsSm/nf/7nf9S1a1cVFRVpzZo12r9/v1599VVFREToj3/8oxcrdrd+/XpNmTJFM2bMUEZGhnr27KkBAwZcdV8WL16s6dOna9asWfrqq680e/ZsTZo0Se+8847bcuHh4crJyXF7BQcHV9u+SFwKDgCohYwxOnfBN3cqDgn0t3TVztmzZ/X6669r165dys3N1apVq/T4449b/t7CwkKNGTNGAwcO1MaNG13tbdq0UZcuXSrs+fGWuXPnauzYsRo3bpwkad68efrggw+0ePFipaSkVLjOq6++qgkTJigxMVGS1LZtW33xxRd6+umnNWjQINdyDodDzZs3r7baK0K4AQDUOuculOrGxz/wyXfvm9NfoU7PT4/r16/X9ddfr+uvv1733HOPHnzwQf3xj3+0fFnzBx98oLy8PE2bNq3Czxs2bHjFdZOSkvTXv/71qtvft2+fWrVqVa69uLhY6enpeuyxx9za+/Xrpx07dlxxe0VFReV6YEJCQrRz505duHBBgYGBkqQzZ84oNjZWpaWl6tChg5544gl17NjxqrVWlc+HpayO8W3evFnx8fEKDg5W27ZttWTJkhqqFACA8pYvX6577rlHknT77bfrzJkz+vjjjy1v58CBA5KkG264wfK6c+bMUWZm5lVf0dHRFa6bl5en0tJSRUZGurVHRkYqNzf3it/Zv39/LVu2TOnp6TLGKC0tTStWrNCFCxeUl5fn2pdVq1bp7bff1tq1axUcHKwePXq49rW6+LTn5tIY36JFi9SjRw+99NJLGjBgwBXTZVZWlgYOHKjx48frr3/9q7Zv366JEyeqadOm+s1vfuODPQAAVIeQQH/tm9PfZ9/tqa+//lo7d+7Uhg0bJEkBAQFKTEzUihUr1KdPH0vfa4yxtPzlmjVrpmbNmlV6fan8DfSMMVftffrjH/+o3Nxcde3aVcYYRUZGavTo0XrmmWdcdxfu2rWrunbt6lqnR48e6tSpk1544QUtWLCgSvVejU/DjdUxviVLlqhVq1aaN2+eJKl9+/ZKS0vTs88+S7gBABtxOByWhoZ8Zfny5SopKVGLFi1cbcYYBQYG6uTJk/rJT36i8PBwSVJ+fn65oaVTp04pIiJCkhQXFydJ+r//+z9169bNUh1VGZZq0qSJ/P39y/XSHD9+vFxvzuVCQkK0YsUKvfTSS/r3v/+tqKgoLV26VA0aNFCTJk0qXMfPz08/+9nPqr3nxmfDUpfG+Pr16+fWfrUxvs8//7zc8v3791daWpouXLhQ4TpFRUUqKChwewEAUFUlJSVavXq1nnvuObfhn3/84x+KjY3VmjVrJEnXXXed/Pz8tGvXLrf1c3JydPToUV1//fWSLp7/mjRpomeeeabC77vahOKqDEs5nU7Fx8crNTXVrT01NVXdu3f/0eMQGBioli1byt/fX+vWrdN//dd/XfFRGsYYZWZmKioq6ke3WxU+i8WVGePLzc2tcPmSkhLl5eVVeLBSUlI0e/Zs7xUOAIAuXrZ98uRJjR071tX7csnQoUO1fPlyPfDAA2rQoIEmTJighx9+WAEBAbr11lt17NgxzZgxQ+3bt3f9oz0sLEzLli3TXXfdpcGDB2vy5Mm69tprlZeXp9dff13Z2dlat25dhbVUdVgqOTlZ9957rxISEtStWzctXbpU2dnZSkpKci0zffp0HT161HUvm2+++UY7d+5Uly5ddPLkSc2dO1dffvmlXnnlFdc6s2fPVteuXXXdddepoKBACxYsUGZmphYuXFjpWj3h8z4/q2N8FS1fUfsl06dPV3Jysut9QUGBYmJiKlvuFV0+PmxlvBYAUDctX75cffr0KRdsJOk3v/mNnnrqKe3evVudOnXS888/r6ioKP3+97/Xv/71LzVr1ky9evXSunXrFBDwn1Pxr371K+3YsUMpKSkaPny465zVu3dvPfnkk9W2L4mJiTpx4oTmzJmjnJwc3Xzzzdq0aZNiY2Ndy+Tk5Ljd96a0tFTPPfecvv76awUGBqpXr17asWOHWrdu7Vrm1KlTuu+++5Sbm6uIiAh17NhRW7ZsUefOnattXyTJYaoyg6kKiouLFRoaqr/97W8aMmSIq/2hhx5SZmamNm/eXG6dn//85+rYsaPmz5/vatu4caOGDRumwsJC12VnV1NQUKCIiAjl5+e7xkEBAL5z/vx5ZWVlua6cRf11td8FK+dvn825qcwYX7du3cot/+GHHyohIcGjYAMAAOzPp/e5SU5O1rJly7RixQrt379fU6dOdRvjmz59ukaOHOlaPikpSYcOHVJycrL279+vFStWaPny5XrkkUd8tQsAAKCW8emcmx8b4/vh+F6bNm20adMmTZ06VQsXLlR0dLQWLFjAZeAAAMDFZ3NufIU5NwBQuzDnBpfU+Tk3AABcrp79WxsV8NbvAOEGAOBTly4IKSws9HEl8LXi4mJJcj2+obJ8fp8bAED95u/vr4YNG+r48eOSpNDQUMtP1EbdV1ZWpu+++06hoaFu9/6pDMINAMDnmjdvLkmugIP6yc/PT61atapyuCXcAAB8zuFwKCoqSs2aNbviswJhf06n84rPpbKCcAMAqDX8/f2rPN8CYEIxAACwFcINAACwFcINAACwlXo35+bSDYIKCgp8XAkAAPDUpfO2Jzf6q3fh5vTp05KkmJgYH1cCAACsOn36tCIiIq66TL17tlRZWZmOHTumBg0aeP0mUQUFBYqJidHhw4d5blU14jjXDI5zzeA41xyOdc2oruNsjNHp06cVHR39o5eL17ueGz8/P7Vs2bJavyM8PJz/cWoAx7lmcJxrBse55nCsa0Z1HOcf67G5hAnFAADAVgg3AADAVgg3XhQUFKSZM2cqKCjI16XYGse5ZnCcawbHueZwrGtGbTjO9W5CMQAAsDd6bgAAgK0QbgAAgK0QbgAAgK0QbgAAgK0QbixatGiR2rRpo+DgYMXHx2vr1q1XXX7z5s2Kj49XcHCw2rZtqyVLltRQpXWbleO8YcMG9e3bV02bNlV4eLi6deumDz74oAarrbus/j5fsn37dgUEBKhDhw7VW6BNWD3ORUVFmjFjhmJjYxUUFKR27dppxYoVNVRt3WX1OK9Zs0a33nqrQkNDFRUVpTFjxujEiRM1VG3dtGXLFg0aNEjR0dFyOBx66623fnQdn5wHDTy2bt06ExgYaF5++WWzb98+89BDD5mwsDBz6NChCpc/ePCgCQ0NNQ899JDZt2+fefnll01gYKB54403arjyusXqcX7ooYfM008/bXbu3Gm++eYbM336dBMYGGh2795dw5XXLVaP8yWnTp0ybdu2Nf369TO33nprzRRbh1XmOA8ePNh06dLFpKammqysLPO///u/Zvv27TVYdd1j9Thv3brV+Pn5mfnz55uDBw+arVu3mptuusnceeedNVx53bJp0yYzY8YM8+abbxpJZuPGjVdd3lfnQcKNBZ07dzZJSUlubTfccIN57LHHKlx+2rRp5oYbbnBrmzBhgunatWu11WgHVo9zRW688UYze/Zsb5dmK5U9zomJieYPf/iDmTlzJuHGA1aP83vvvWciIiLMiRMnaqI827B6nP/yl7+Ytm3burUtWLDAtGzZstpqtBtPwo2vzoMMS3mouLhY6enp6tevn1t7v379tGPHjgrX+fzzz8st379/f6WlpenChQvVVmtdVpnj/ENlZWU6ffq0GjVqVB0l2kJlj/PKlSv17bffaubMmdVdoi1U5ji//fbbSkhI0DPPPKMWLVooLi5OjzzyiM6dO1cTJddJlTnO3bt315EjR7Rp0yYZY/Tvf/9bb7zxhu64446aKLne8NV5sN49OLOy8vLyVFpaqsjISLf2yMhI5ebmVrhObm5uhcuXlJQoLy9PUVFR1VZvXVWZ4/xDzz33nM6ePathw4ZVR4m2UJnjfODAAT322GPaunWrAgL40+GJyhzngwcPatu2bQoODtbGjRuVl5eniRMn6vvvv2fezRVU5jh3795da9asUWJios6fP6+SkhINHjxYL7zwQk2UXG/46jxIz41FDofD7b0xplzbjy1fUTvcWT3Ol6xdu1azZs3S+vXr1axZs+oqzzY8Pc6lpaUaPny4Zs+erbi4uJoqzzas/D6XlZXJ4XBozZo16ty5swYOHKi5c+dq1apV9N78CCvHed++fZo8ebIef/xxpaen6/3331dWVpaSkpJqotR6xRfnQf755aEmTZrI39+/3L8Cjh8/Xi6VXtK8efMKlw8ICFDjxo2rrda6rDLH+ZL169dr7Nix+tvf/qY+ffpUZ5l1ntXjfPr0aaWlpSkjI0MPPPCApIsnYWOMAgIC9OGHH6p37941UntdUpnf56ioKLVo0UIRERGutvbt28sYoyNHjui6666r1prrosoc55SUFPXo0UOPPvqoJOmWW25RWFiYevbsqSeffJKedS/x1XmQnhsPOZ1OxcfHKzU11a09NTVV3bt3r3Cdbt26lVv+ww8/VEJCggIDA6ut1rqsMsdZuthjM3r0aL322muMmXvA6nEODw/X3r17lZmZ6XolJSXp+uuvV2Zmprp06VJTpdcplfl97tGjh44dO6YzZ8642r755hv5+fmpZcuW1VpvXVWZ41xYWCg/P/dToL+/v6T/9Cyg6nx2HqzW6co2c+lSw+XLl5t9+/aZKVOmmLCwMPOvf/3LGGPMY489Zu69917X8pcugZs6darZt2+fWb58OZeCe8DqcX7ttddMQECAWbhwocnJyXG9Tp065atdqBOsHucf4mopz1g9zqdPnzYtW7Y0Q4cONV999ZXZvHmzue6668y4ceN8tQt1gtXjvHLlShMQEGAWLVpkvv32W7Nt2zaTkJBgOnfu7KtdqBNOnz5tMjIyTEZGhpFk5s6dazIyMlyX3NeW8yDhxqKFCxea2NhY43Q6TadOnczmzZtdn40aNcrcdtttbst/9tlnpmPHjsbpdJrWrVubxYsX13DFdZOV43zbbbcZSeVeo0aNqvnC6xirv8+XI9x4zupx3r9/v+nTp48JCQkxLVu2NMnJyaawsLCGq657rB7nBQsWmBtvvNGEhISYqKgoM2LECHPkyJEarrpu+fTTT6/697a2nAcdxtD/BgAA7IM5NwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwDcrFq1Sg0bNvR1GZXWunVrzZs376rLzJo1Sx06dKiRegDUPMINYEOjR4+Ww+Eo9/rnP//p69K0atUqt5qioqI0bNgwZWVleWX7u3bt0n333ed673A49NZbb7kt88gjj+jjjz/2yvddyQ/3MzIyUoMGDdJXX31leTt1OWwCvkC4AWzq9ttvV05OjturTZs2vi5L0sWnjOfk5OjYsWN67bXXlJmZqcGDB6u0tLTK227atKlCQ0Ovusw111yjxo0bV/m7fszl+/nuu+/q7NmzuuOOO1RcXFzt3w3UZ4QbwKaCgoLUvHlzt5e/v7/mzp2rn/70pwoLC1NMTIwmTpyoM2fOXHE7//jHP9SrVy81aNBA4eHhio+PV1pamuvzHTt26Oc//7lCQkIUExOjyZMn6+zZs1etzeFwqHnz5oqKilKvXr00c+ZMffnll66epcWLF6tdu3ZyOp26/vrr9eqrr7qtP2vWLLVq1UpBQUGKjo7W5MmTXZ9dPizVunVrSdKQIUPkcDhc7y8flvrggw8UHBysU6dOuX3H5MmTddttt3ltPxMSEjR16lQdOnRIX3/9tWuZq/08PvvsM40ZM0b5+fmuHqBZs2ZJkoqLizVt2jS1aNFCYWFh6tKliz777LOr1gPUF4QboJ7x8/PTggUL9OWXX+qVV17RJ598omnTpl1x+REjRqhly5batWuX0tPT9dhjjykwMFCStHfvXvXv31+//vWvtWfPHq1fv17btm3TAw88YKmmkJAQSdKFCxe0ceNGPfTQQ3r44Yf15ZdfasKECRozZow+/fRTSdIbb7yh559/Xi+99JIOHDigt956Sz/96U8r3O6uXbskSStXrlROTo7r/eX69Omjhg0b6s0333S1lZaW6vXXX9eIESO8tp+nTp3Sa6+9Jkmu4ydd/efRvXt3zZs3z9UDlJOTo0ceeUSSNGbMGG3fvl3r1q3Tnj17dNddd+n222/XgQMHPK4JsK1qf+44gBo3atQo4+/vb8LCwlyvoUOHVrjs66+/bho3bux6v3LlShMREeF636BBA7Nq1aoK17333nvNfffd59a2detW4+fnZ86dO1fhOj/c/uHDh03Xrl1Ny5YtTVFRkenevbsZP3682zp33XWXGThwoDHGmOeee87ExcWZ4uLiCrcfGxtrnn/+edd7SWbjxo1uy8ycOdPceuutrveTJ082vXv3dr3/4IMPjNPpNN9//32V9lOSCQsLM6GhoUaSkWQGDx5c4fKX/NjPwxhj/vnPfxqHw2GOHj3q1v7LX/7STJ8+/arbB+qDAN9GKwDVpVevXlq8eLHrfVhYmCTp008/1VNPPaV9+/apoKBAJSUlOn/+vM6ePeta5nLJyckaN26cXn31VfXp00d33XWX2rVrJ0lKT0/XP//5T61Zs8a1vDFGZWVlysrKUvv27SusLT8/X9dcc42MMSosLFSnTp20YcMGOZ1O7d+/321CsCT16NFD8+fPlyTdddddmjdvntq2bavbb79dAwcO1KBBgxQQUPk/ZyNGjFC3bt107NgxRUdHa82aNRo4cKB+8pOfVGk/GzRooN27d6ukpESbN2/WX/7yFy1ZssRtGas/D0navXu3jDGKi4tzay8qKqqRuURAbUe4AWwqLCxM1157rVvboUOHNHDgQCUlJemJJ55Qo0aNtG3bNo0dO1YXLlyocDuzZs3S8OHD9e677+q9997TzJkztW7dOg0ZMkRlZWWaMGGC25yXS1q1anXF2i6d9P38/BQZGVnuJO5wONzeG2NcbTExMfr666+Vmpqqjz76SBMnTtRf/vIXbd682W24x4rOnTurXbt2Wrdune6//35t3LhRK1eudH1e2f308/Nz/QxuuOEG5ebmKjExUVu2bJFUuZ/HpXr8/f2Vnp4uf39/t8+uueYaS/sO2BHhBqhH0tLSVFJSoueee05+fhen3L3++us/ul5cXJzi4uI0depU/fa3v9XKlSs1ZMgQderUSV999VW5EPVjLj/p/1D79u21bds2jRw50tW2Y8cOt96RkJAQDR48WIMHD9akSZN0ww03aO/everUqVO57QUGBnp0Fdbw4cO1Zs0atWzZUn5+frrjjjtcn1V2P39o6tSpmjt3rjZu3KghQ4Z49PNwOp3l6u/YsaNKS0t1/Phx9ezZs0o1AXbEhGKgHmnXrp1KSkr0wgsv6ODBg3r11VfLDZNc7ty5c3rggQf02Wef6dChQ9q+fbt27drlChr//d//rc8//1yTJk1SZmamDhw4oLffflsPPvhgpWt89NFHtWrVKi1ZskQHDhzQ3LlztWHDBtdE2lWrVmn58uX68ssvXfsQEhKi2NjYCrfXunVrffzxx8rNzdXJkyev+L0jRozQ7t279ac//UlDhw5VcHCw6zNv7Wd4eLjGjRunmTNnyhjj0c+jdevWOnPmjD7++GPl5eWpsLBQcXFxGjFihEaOHKkNGzYoKytLu3bt0tNPP61NmzZZqgmwJV9O+AFQPUaNGmV+9atfVfjZ3LlzTVRUlAkJCTH9+/c3q1evNpLMyZMnjTHuE1iLiorM3XffbWJiYozT6TTR0dHmgQcecJtEu3PnTtO3b19zzTXXmLCwMHPLLbeYP/3pT1esraIJsj+0aNEi07ZtWxMYGGji4uLM6tWrXZ9t3LjRdOnSxYSHh5uwsDDTtWtX89FHH7k+/+GE4rfffttce+21JiAgwMTGxhpjyk8ovuRnP/uZkWQ++eSTcp95az8PHTpkAgICzPr1640xP/7zMMaYpKQk07hxYyPJzJw50xhjTHFxsXn88cdN69atTWBgoGnevLkZMmSI2bNnzxVrAuoLhzHG+DZeAQAAeA/DUgAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFYINwAAwFb+Py3voOF45ui1AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.8995\n",
      "Test F1 : 0.8975\n",
      "AUC: 0.9545351473922903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# 검증 루프\n",
    "outputs = model(X_test_tensor)\n",
    "y_pred = (outputs > 0.5).float()\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, outputs.detach().numpy())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ROC 곡선 그리기\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc).plot()\n",
    "plt.show()\n",
    "print(f'Test Accuracy : {accuracy:.4f}')\n",
    "print(f'Test F1 : {f1:.4f}')\n",
    "print(\"AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed86e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
